---
title: "Monitoria Variables Instrumentales"
author: "Juan C. Forero - Jhan Andrade - Germán C. Rodriguéz"
date: "7/10/2020"
output: pdf_document
citation_package: biblatex
biblio-style: apa
biblatexoptions: [backend=biber, maxbibnames=999]
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\tableofcontents

> El presente documento fue creado utilizando R markdown. La idea es ilustrar los prinipales conceptos de variables instrumentales mediante el software R

# Introducción 

## Teoría para el caso univariado

El método de estimación por **Variables Instrumentales** nos permite resolver el problema de endogeneidad que pueda presentar algún regresor\footnote{En el presente documento, una variable se va a considerar endogena si existe una correlación entre dicha varaible en cuestión y el error $\varepsilon$}.

Ahora bien, este método nos permite obtener estimadores consistentes\footnote{Fijense que en el curso se hace un mayor enfásis en la consistencia de los estiamdores que en la insesgadez. Ello se debe a que requerir que los estiamdores sean insesgados puede ser una condición muy fuerte. No obstante, la consistencia es una propiedad que no es tan restrictiva y que es fundamental que tengan cualquier estimador que se desea utilizar en la práctica}. Este método de estimación está basado en encontrar un instrumento (*Z*) que cumpla las siguientes condiciones: 

- **Relevancia**: $cov(Z,X)\neq 0$\footnote{X sería el regresor que se considera endógeno y por ende es necesario el uso de un instrumento para corregir por ese problema de endogeneidad}
- **Exogeneidad**: $cov (Z,\varepsilon)) = 0$

El **supuesto de relevancia** busca que el instrumento empleado *Z* tenga un alto grado de correlación con el regresor *X* que se considera endógeno. Entre mayor correlación entre el instrumento que se empleará y el regresor endógeno mejor. Diremos que un instrumento con un alto grado de correlación con el regresor es un instrumento fuerte para éste, de lo contrario diremos que es débil. El **supuesto de exogeneidad** requiere que el instrumento no tenga correlación con el error. 

Ahora bien, a pesar de que teóricamente es muy importante que los dos supuestos se cumplan para que las estimaciones por **VI** sean válidas, en la practica se tiene que solo el supuesto de relevancia se puede corroborar. Lo anterior, quiere decir que **no existen** pruebas estadísticas capaces de corroborar el supuesto de **exogeneidad**, la justificación en la práctica de dicho supuesto se hace desde la *teoría económica* o desde la *lógica del modelo* que justifique la exogeneidad del instrumento. 

Es importante tener en cuenta, que el método de variables instrumentales es bastante sencillo, sin embargo encontrar buenos instrumentos es la parte compleja del método. En la prática, un investigador que vaya emplear el método de variables instrumentales dedicará la mayor parte del tiempo a buscar y justificar porque su instrumento es adecuado para la estimaciónd dado que el cumplimiento de los dos supuestos mencioandos anteriormente es fundamental para una buena estimación por VI.

## MC2E

En la práctica, es muy usual emplear variables instrumentales mediante lo que se conoce como **Mínimos cuadrados en 2 etapas (MC2E)**.

Para el ejemplo ilustrativo de *MC2E*\footnote{Para justificar el método de MC2E se debe satisfacer las condiciones de orden y rango. La de orden requiere que haya al menos tantos instrumentos como regresores endógenas y la condición de rango es una condición más elaborada que se encuentra por ejemplo en el libro avanado de Wooldridge.}, nos basaremos en [@wooldridge2016introductory]

Suponga que se tiene la siguiente ecuación estructural\footnote{Recuerden que una ecuación estructural es aquella en la que la variable de interés se escribe tanto en los regresores endógenos como exógenos}: $y_1 = \beta_0 + \beta_1 y_2 + \beta_2 z_1 + \nu_1$ y suponga que además $y_2$ es una variable endógena con dos instrumentos $z_2$ y $z_3$. El método consiste en dos etapas en donde en cada etapa se hace una estimación por OLS.

### Primera etapa

En la primera etapa, hay que estimar por OLS la ecuación en forma reducida\footnote{Recuerden que una ecuación en forma reducida es aquella en la que una variable endógena se escribe a partir de variables exógenas exclusivamente}: 

$$
y_2 = \pi_0 + \pi_1 z_1 + \pi_2 z_2 + \pi_3 z_3 + \nu_2
$$
Dónde para que las variables $Z_i$ sean exógenas se requiere que: $cov(z_1, \nu_2) = cov(z_2, \nu_2) = cov(z_3, \nu_2) = 0$. 

Se busca que los instrumentos sean relevantes, es decir, que $\pi_2 \neq 0$ o $\pi_3 \neq 0$. Por tanto, de las siguientes hipótesis nulas: 

\begin{align*}
  H_0&: \pi_2 = \pi_3 = 0 \\
  H_a&: \pi_2 \neq 0 \quad o \quad \pi_3 \neq 0 \\
\end{align*}

Se emplea un *estadístico F* para realizar las pruebas de hipótesis anteriores y lo que se busca es rechazar la hipótesis nula a favor de la alternativa para corroborar relevancia. 

Luego, de mirar la relevancia de los instrumentos y justificar que los instrumentos son exógenos se procede a realizar la segunda etapa.

### Segunda etapa

Dados los resultados de la estimación, es decir $\hat{y_2} = \hat{\pi_0} + \hat{\pi_1} z_1 + \hat{\pi_2} z_2 + \hat{\pi_3} z_3$, se reemplaza $y_2$ por $\hat{y_2}$\footnote{Recordar que en el ejemplo $y_2$ es el regresor endógeno} y se estima la ecuación estructural:
$$
y_1 = \beta_0 + \beta_1 \hat{y_2} + \beta_2 z_1 + \nu_1
$$
por *OLS*\footnote{Recordar que en el ejemplo $z_1$ no es un instrumento}. Los parámetros estimados por el procedimiento anterior, de dos etapas se le conoce como los *parámetros estimados por MC2E*. 

## Paquete para realizar variables instrumentales en R

En R Studio instalaremos los siguientes paquetes para proceder a realizar el ejercicio de Variables Instrumentales. 

```{r}
#install.packages("AER") #Applied Econometrics with R for Instrumental Variables
#install.packages("foreing") #Para cargar datos con formato Stata
#install.packages("stargazer") #Para una presentación más estética de los resultados
#install.packages("estimatr") #Para hacer MC2E con errores robustos
#install.packages("arm") #Análisis de datos utilizando regresiones
#install.packages("lmtest")
```

Cargamos los paquetes: 

```{r echo=TRUE, message=FALSE}  
library(AER);library(foreign); library(stargazer); 
library(arm);library(lmtest);library(estimatr);library(tidyverse)
```

- **AER**: Applied Econometrics with R for Instrumental Variables. Contiene la función *ivreg*
- **foreing**: Para importar bases de datos tipo stata *dta*
- **stargazer**: Para prestnación de resultados 
- **estimatr**: Para hacer MC2E con errores robustos. Contiene la función *iv_robust*
- **arm**: Para análisis de datos utilizando regresiones
- **lmtest**: Conjutntos de test para modelos de estimación lineales 

# Ejemplo Univariado (un solo instrumento) (Para el salario determiando por la educación)

```{r}
#Cargamos la base de datos. 
data=read.dta("http://fmwww.bc.edu/ec-p/data/wooldridge/mroz.dta")
```

La estructura de datos es de corte transversal y corresponde a un data.frame de 753 para 22 variables:
```{r, cache=TRUE}
glimpse(data)
```

Las principales variables son: 

- inlf: =1 if in lab frce, 1975;
- hours: hours worked, 1975;
- kidslt6: kids < 6 years kidsge6: # kids 6-18;
- age: woman's age in yrs;
- educ: years of schooling;
- wage: est. wage from earn, hrs;
- repwage: rep. wage at interview in 1976;
- hushrs: hours worked by husband, 1975;
- husage: husband's age;
- huseduc: husband's years of schooling;
- huswage: husband's hourly wage, 1975;
- faminc: family income, 1975;
- mtr: fed. marg. tax rte facing woman;
- motheduc: mother's years of schooling;
- fatheduc: father's years of schooling;
- unem: unem. rate in county of resid;
- city: =1 if live in SMSA;
- exper: actual labor mkt exper;
- nwifeinc: (faminc - wage*hours)/1000;
- lwage: log(wage);
- expersq: exper^2

Ahora vamos a eliminar las observaciones que no tienen salario: !is.na(wage)

```{r}
data.VI <- subset(data,!is.na(wage))
attach(data.VI)
```

Se busca explicar la variación de salario en función de la educación de las personas. No obstante, es de esperarse que la educación sea una variable endógena dado que dependerá de factores inobservables de las personas como su habilidad innata u otros factores no observables que se recogen en el error. 

Por tanto, utilizando Mínimos Cuadrados Ordinarios (MCO), sin ningun consideración adicional, se espera que en este procedimiento existan problemas de endogeneidad: 

```{r, cache=TRUE}
R.MCO = lm(log(wage)~ educ, data=data.VI)
summary(R.MCO)
```

Usaremos el método de variables instrumentales, de tal manera que la educación del padre *fatheduc* será un instrumento para *educ*. Pues, es de esperarse que la educación de los padres determine la educación de los hijos. Entre más años de escolaridad tengan los padres de las personas, los hijos tengan un mayor grado de escolaridad. Es decir, una persona cuyos padres tienen un nivel de educación básica-primaria posiblemente tenga un nivel de escolaridad menor que el de una persona cuyos padres tienen doctorado. 

Empezaremos calculando el coeficiente $\beta_1$ de forma manual \footnote{Acá se está empleando la fórmula de variable instrumental cuándo solo hay un instrumento}:
 
 $$\frac{cov(y,z)}{cov(x,z)}$$
 
```{r, cache=TRUE}
Beta = with(data.VI,cov(log(wage),fatheduc)/cov(educ,fatheduc)); Beta
```
 
## Mínimos Cuadrados en 2 Etapas

### Primera etapa 

En esta etapa se deben incluir como controles todas las variables exógenas del modelo. También debe evaluarse la relevancia del instrumento. 

```{r, cache=TRUE}
Reg.aux=lm(educ ~ fatheduc, data = data.VI); summary(Reg.aux)
```

De lo anterior, vemos que **nuestro instrumento *fatheduc* es estadísticamente significativo y por lo tanto se puede concluir que se cumple el supuesto de relevancia**

Calculamos los valores ajustados de la regresión de la primera etapa para la variable *educ*, que harán de instrumento en la segunda etapa.

```{r, cache=TRUE}
educ.fitted = fitted(Reg.aux)
```

### Segunda etapa 

Incluimos los valores ajustados de educ que se obtuvieron de la primera etapa: 

```{r, cache=TRUE}
Reg.VI = lm(log(wage)~educ.fitted, data=data.VI) 
summary(Reg.VI)
```

## Usando el comando *ivreg* 

El comando ***ivreg*** del paquete **AER** nos permite realizar el procedimiento de Variables instrumentales sin necesidad de hacer etapa por etapa. En otras palabras, este comando nos ahorra lineas de código. 

```{r, cache=TRUE}
R.VI = ivreg(log(wage) ~ educ| fatheduc , data=data.VI); summary(R.VI)
```

La estructura del código consiste en: 

ivreg(Variable Explicada ~ regresores del modelo (incluyendo regresores endógenos y exógenos) **|** Variables exógenas + instrumentos)

## Código *iv_robust* 

El comando ***iv_robust*** del paquete *estimatr* incorpora los errores estándar robustos. La estructura de código es la misma que la de *ivreg*. La principal diferencia es que con *iv_robust* el R^2 se corrige y es posible hacer una adecuada interpretación de este\footnote{Por lo anterior, se recomienda que utilizen \textbf{iv\_robuts} a la hora de realizar estiamciones por *MC2E*}.

```{r}
R.VI.Robust = iv_robust(log(wage) ~ educ| fatheduc , data=data.VI);summary(R.VI.Robust)
```

Presentando los resultados mediante el comando stargazer\footnote{Fijense que una de las principales débilidades del comando \textbf{iv\_robust} es que no se puede usar directamente en el stargazer.}. La tabla generada en latex por el paquete stargazer es: 

```{r echo=FALSE, include=FALSE, cache=TRUE}
stargazer(R.MCO,Reg.VI,Reg.VI, column.labels=c("MCO","MC2E", "IVREG"), style = "AER")
```

\begin{table}[!htbp] \centering 
  \caption{Tabla de regresiones para el ejemplo univariado} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{3}{c}{\textit{Dependent variable:}} \\ 
\cline{2-4} 
\\[-1.8ex] & \multicolumn{3}{c}{log(wage)} \\ 
 & MCO & MC2E & IVREG \\ 
\\[-1.8ex] & (1) & (2) & (3)\\ 
\hline \\[-1.8ex] 
 educ & 0.109$^{***}$ &  &  \\ 
  & (0.014) &  &  \\ 
  & & & \\ 
 educ.fitted &  & 0.059 & 0.059 \\ 
  &  & (0.037) & (0.037) \\ 
  & & & \\ 
 Constant & $-$0.185 & 0.441 & 0.441 \\ 
  & (0.185) & (0.467) & (0.467) \\ 
  & & & \\ 
\hline \\[-1.8ex] 
Observations & 428 & 428 & 428 \\ 
R$^{2}$ & 0.118 & 0.006 & 0.006 \\ 
Adjusted R$^{2}$ & 0.116 & 0.004 & 0.004 \\ 
Residual Std. Error (df = 426) & 0.680 & 0.722 & 0.722 \\ 
F Statistic (df = 1; 426) & 56.929$^{***}$ & 2.586 & 2.586 \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{3}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

Llegados a este punto, se pueden dar cuenta que para el caso univariado la estimación manual por covarianzas, Mínimos Cuadrados Ordinarios en 2 etapas, IVREG y IV_ROBUST nos llevan al mismo coeficiente. (0.059)

# Ejemplo multivariado(Dos instrumentos) (Para el salario determiando por la educación)

Estimaremos la regresión MCO que originalmente debe tener problemas de endogenidad\footnote{Por la misma razón que en el ejemplo anterio, muy seguramente la variable educación está correlacionada con el error (i.e. con factores inobservables)}. 

```{r, cache=TRUE}
Reg.MCO = lm(log(wage)~educ+exper+I(exper^2)+city, data=data.VI)
summary(Reg.MCO)
```

Al igual que con el ejemplo univariado, es de esperarse que la educación sea endógena en la regresión. Para resolver este problema tomaremos la ecuación del padre (*fatheduc*) y la educación de la madre (*motheduc*) como instrumentos para *educ*\footnote{Fijense que se puede tener más de un instrumento para la misma variable endógena pero al menos se necesita un instrumento, lo anterior dado por la condción de orden para \textit{MC2E}}. 

Vamos a realizar un análisis preliliminar la relevancia de los instrumentos, de tal manera que la covarianza entre *educ* y el instrumento debe ser diferente de cero. 

```{r, cache=TRUE}
cor(educ,motheduc)
cor(educ,fatheduc) 
```

## Mínimos cuadrados en 2 etapas 
### Primera etapa 

Recuerden que en esta etapa se hace la regresión de la variable endógena (*educ*) en función de las exógenas y los instrumentos. 

```{r, cache=TRUE}
stage1 <- lm(educ~exper+I(exper^2)+city+motheduc+fatheduc, data=data.VI)
```

### Segunda etapa

```{r, cache=TRUE}
stage2<-lm(log(wage)~fitted(stage1)+exper+I(exper^2)+city, data=data.VI)
summary(stage2)
```

## Ivreg 

ivreg(Variable Explicada ~ regresores del modelo (incluyendo regresores endógenos y exógenos) **|** Variables exógenas + instrumentos, data="...")

**OJO**: recuerden que la forma en cómo funciona el comando *ivreg* es: ivreg(Variable Explicada ~ regresores del modelo (incluyendo regresores endógenos y exógenos) **|** Variables exógenas + instrumentos, data="...")\footnote{Adicionalmente es importante que tengan en cuenta que "|" no es una L ni una I, es una raya vertical que por lo general se encuentra en la esquina superior izquierda de los teclados (debajo del esc). La barra vertical nos ayudará a indicarle al código cuales son nuestras variables exógenas y cuál es la endógena.}

```{r, cache=TRUE}
aut.MC2E<-ivreg(log(wage)~educ+exper+I(exper^2)+
                  city|motheduc+fatheduc+exper+I(exper^2) + city , data=data.VI)
summary(aut.MC2E)
```

## iv_robust 

El comando *iv_robust*\footnote{El método que utiliza por default iv\_robuts para corregir por errores robustos es el método que se conoce como \textbf{HC2}, como lo pueden observar en el summary} funciona con la misma sintaxis que *ivreg*, tienen la misma estructura de código. 

```{r, cache=TRUE}
MC2E.Robusto <- iv_robust(log(wage)~educ+exper+I(exper^2)+
                            city|motheduc+fatheduc+exper+I(exper^2) + city , data=data.VI)
summary(MC2E.Robusto)
```

Presentación de resultados\footnote{Nuevamente, se resalta que a pesar de que se sugiere emplear iv\_robuts para hacer las estiamciones de MC2E se tiene que no es posible incluir los resultados de dicho estimación directamente en stargazer}:

```{r echo=FALSE, include=FALSE, cache=TRUE}
stargazer(Reg.MCO, stage1,stage2,aut.MC2E,
          column.labels = c("MCO", "Etapa 1","Etapa 2","IVREG"))
```

\begin{table}[!htbp] \centering 
  \caption{Resultados de la estimación para el caso de dos instrumentos} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lcccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & log(wage) & educ & \multicolumn{2}{c}{log(wage)} \\ 
\\[-1.8ex] & \textit{OLS} & \textit{OLS} & \textit{OLS} & \textit{instrumental} \\ 
 & \textit{} & \textit{} & \textit{} & \textit{variable} \\ 
 & MCO & Etapa 1 & Etapa 2 & IVREG \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4)\\ 
\hline \\[-1.8ex] 
 educ & 0.106$^{***}$ &  &  & 0.055$^{*}$ \\ 
  & (0.014) &  &  & (0.033) \\ 
  & & & & \\ 
 fitted(stage1) &  &  & 0.055 &  \\ 
  &  &  & (0.034) &  \\ 
  & & & & \\ 
 exper & 0.041$^{***}$ & 0.040 & 0.043$^{***}$ & 0.043$^{***}$ \\ 
  & (0.013) & (0.040) & (0.014) & (0.013) \\ 
  & & & & \\ 
 I(exper$\hat{\mkern6mu}$2) & $-$0.001$^{**}$ & $-$0.001 & $-$0.001$^{**}$ & $-$0.001$^{**}$ \\ 
  & (0.0004) & (0.001) & (0.0004) & (0.0004) \\ 
  & & & & \\ 
 city & 0.054 & 0.467$^{**}$ & 0.092 & 0.092 \\ 
  & (0.068) & (0.209) & (0.076) & (0.072) \\ 
  & & & & \\ 
 motheduc &  & 0.164$^{***}$ &  &  \\ 
  &  & (0.036) &  &  \\ 
  & & & & \\ 
 fatheduc &  & 0.174$^{***}$ &  &  \\ 
  &  & (0.034) &  &  \\ 
  & & & & \\ 
 Constant & $-$0.531$^{***}$ & 8.914$^{***}$ & 0.072 & 0.072 \\ 
  & (0.199) & (0.433) & (0.422) & (0.404) \\ 
  & & & & \\ 
\hline \\[-1.8ex] 
Observations & 428 & 428 & 428 & 428 \\ 
R$^{2}$ & 0.158 & 0.221 & 0.056 & 0.133 \\ 
Adjusted R$^{2}$ & 0.150 & 0.211 & 0.047 & 0.125 \\ 
Residual Std. Error & 0.667 (df = 423) & 2.029 (df = 422) & 0.706 (df = 423) & 0.676 (df = 423) \\ 
F Statistic & 19.856$^{***}$ (df = 4; 423) & 23.901$^{***}$ (df = 5; 422) & 6.223$^{***}$ (df = 4; 423) &  \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{4}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table}

# Test de diagnósticos: Hausman, Instrumentos débiles y sobreidentificación de Sargan

## Test de Hausman 

Algo que es muy impotante es saber si un regresor es endógeno o no lo es. Lo anterior se debe a que si el regresor no fuera endógeno entonces la estimación habiltual por MCO sería muchisimo más eficiente que la estimación por VI. Es decir, uno debería realizar una estiamción por VI exclusivamente cuando hay presencia de regresores endógenos, de lo contrario, si todos los regresores fueran exógenoes entonces variables instrumentales sería infeciente frente a MCO. 

El problema de la endogeneidad de un regresor es un problema más teórico que práctica. Es decir, en el ejemplo anterior uno esperaría desde la teoría que la educación se encuentre correlacionada con factores inobservables que hagan parte del error como lo es la habilidad innata de los individuos dado que se espera teóricamente que personas más habilidosas generlamente o en promedio tengan mayores niveles de educación que personas menos habilidosas. 

No obstante, a pesar de que la pregunta es de carácter teórica, existe una prueba formal que podría ayudar a iluminar un poco si algún regresor presenta endógenidad. Dicha prueba consiste como el test de Hausman y busca determinar si un regresor es endógeno o no. 

El test de Hausman dice que: 

\begin{align*}
H_0&=\; \text{exogeneidad de los regresores}
H_a&=\; \text{al menos un regresor es endógeno}
\end{align*}

Esta prueba nos indicará si es necesario o no el uso de variables instrumentales de tal manera que la **hipotesis nula** es  Exógeneidad. De tal manera, que si rechazamos la hipotesis nula, a favor de la alternativa significa que debemos hacer uso de Variables instrumentales. 

### Test de Hausman manual 

La primera forma de aplicar el test de Hausman es realizar el test de manera manual. Este test consiste en dos etapas: 

#### Etapa 1

Para aplicar esta prueba, es necesario calcular los errores o residuos de la **primera etapa**

```{r, cache=TRUE}
res.stage1<-resid(stage1)
```

Ello se debe a que si el regresor fuera endógeno, entonces la *parte endógena* del regresor quedaría *almacenada* en los *residuales de la primera etapa*. 

#### Etapa 2

Luego, estimaremos la regresión aumentada de la Variable dependiente contra los *regresores exógenas originales*, *la variable regresora que se cree endógena* y los *residuales de la primera etapa*. Si encontramos que el coeficiente que acompaña a los residuales de la primera etapa es *estadísticamente significativos* concluiremos que el modelo *sufre de endogeneidad*. 

```{r, cache=TRUE}
Reg.aum <- lm(log(wage)~educ+exper+I(exper^2)+city+res.stage1, data=data.VI)
summary(Reg.aum)
```

De lo anterior, concluimos que a un nivel de significancia del 10 % el test confirma nuestra intuición de que la variable educación es endógena. 

## Test de diagnóstico con base en el modelo de errores robustos.

De igual forma, cuando se tiene un sistema sobreidentificado, también es conveniente realizar algunos test de diagnóstico para corroborar ciertas propiedades de los instrumentos y de los regresores del modelo. Para realizar dichos test de diagnóstico es necesario agregar *diagnostics=TRUE* al comando iv_robust. 

```{r, cache=TRUE}
MC2E.Robusto <- iv_robust(log(wage)~educ+exper+
                            I(exper^2)+city|motheduc+fatheduc+exper+I(exper^2) + city ,
                          data=data.VI, diagnostics=TRUE)
summary(MC2E.Robusto, diagnostics = TRUE)
```

Con este comando se podrá determinar si los instrumento son débiles, si al menos un regresor es endógeno en el modelo y si los instrumentos son exógenos. En ese orden los test de diagnóstico sería, respectivamente: 

+ **Weak instrument:** Ho= Los instrumentos son débiles Ha= al menos un instrumento es débil (no se correlaciona con la variable endógena)
+ **Hausman:** Ho = No hay endogeneidad (si se rechaza hay que usar variables instrumentales (i.e. MC2E))
+ **Sargan:**  Ho= Instrumentos exógenos - Ha= al menos uno es endógeno (sistema sobre-identificado)\footnote{El test de sobreidentificación solo aparece en sistemas sobreidentificados, es decir, cuando hayan más instrumentos que regresores endógenos}

De los resultados del código anterior, es posible concluir que el modelo instrumentado: 

- Del test de *Weak instruments* no se rechaza por lo que se podría decir que ambos instrumentos son fuertes (i.e. tienen fuera correlación con la variable que se supone endógena (la educación)). Con eso se estaría corroborando el supuesto de relvancia para los dos instrumentos

- El test de Hausman es el mismo test que se realizó en la sección anterior solo que acá se hace de manera automática y se corrige por errores robustos. Como se rechaza la hipótesis nula a un nivel de significancia del 10 % entonces se puede decir que hay endógenidad en el regresor. 

- No rechazo la hipótesis nula de que ambos instrumentos son exógenos 

Por los resultados anterioes, en la práctica podría utilizar ambos instrumentos *fatheduc* y  *motheduc* para la variable regresora endógena *educ*

# Bibliografía